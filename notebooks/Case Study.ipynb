{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443c6dcb",
   "metadata": {},
   "source": [
    "# Case Study\n",
    "\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cada4e",
   "metadata": {},
   "source": [
    "### Git & version control\n",
    "\n",
    "1. Create a Github repository called \"ames-housing-analysis\".\n",
    "1. Copy the ames.csv data from the `data/` directory into this repository.\n",
    "1. Update the README with a short synopsis of this repo.\n",
    "1. Create a folder called `notebooks/`\n",
    "1. Add, commit, and push what you have so far. Verify in that it appears in GitHub on your repository page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43125c4",
   "metadata": {},
   "source": [
    "### Exploratory data analysis\n",
    "\n",
    "1. In the repo's `notebooks/` folder, create a new notebook: `eda.ipynb`.\n",
    "2. Load the ames.csv data.\n",
    "3. Assess the distribution of the response variable (`Sale_Price`).\n",
    "4. How many features are numeric vs. categorical?\n",
    "5. Pick a numeric feature that you believe would be influential on a home's `Sale_Price`. Assess the distribution of the numeric feature. Assess the relationship between that feature and the `Sale_Price`.\n",
    "6. Pick a categorical feature that you believe would be influential on a home's `Sale_Price`. Assess the distribution of the categorical feature. Assess the relationship between that feature and the `Sale_Price`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db6075",
   "metadata": {},
   "source": [
    "### Modular code & Scikit-learn model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626052ab",
   "metadata": {},
   "source": [
    "1. Copy `my_module.py` (that we created together) into the notebooks folder.\n",
    "2. Import your module and use `get_features_and_target` to load the numeric features of the Ames data, along with the \"Sale_Price\" as a target column.\n",
    "\n",
    "With your features and target prepared:\n",
    "1. Split the data into training and test sets. Use 75% of the data for training and 25% for testing.\n",
    "2. Fit a default `sklearn.neighbors.KNeighborsRegressor` model on the training data and score on the test data. Note that scoring on regression models provides the $R^2$.\n",
    "3. Fit a default `sklearn.linear_model.LinearRegression` model on the training data and score on the test data.\n",
    "4. Fit a default `sklearn.ensemble.RandomForestRegressor` model on the training data and score on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737cd83",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b0caf",
   "metadata": {},
   "source": [
    "1. Fill in the blanks to standardize the numeric features and then apply a linear regression model. Does standardizing the numeric features improve the linear regression's $R^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6be7a3",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import ________\n",
    "\n",
    "lm_model_scaled = make_pipeline(__________, LinearRegression())\n",
    "lm_model_scaled.fit(X_train, y_train)\n",
    "lm_model_scaled.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57871c5",
   "metadata": {},
   "source": [
    "2. Using the code chunks below, which computes the following:\n",
    "\n",
    "- identifies numeric, categorical, and ordinal columns in our full feature set,\n",
    "- replaces unique values in our ordinal columns (i.e. \"No_basement\", \"No_garage\"), and\n",
    "- creates our encoders for the numeric, categorical, and ordinal columns.\n",
    "\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "    <p class=\"first admonition-title\" style=\"font-weight: bold;\"><b>Note</b></p>\n",
    "<p class=\"last\">Run the following two code cells without changing anything.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0cf7c8",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "######## RUN THIS CODE CELL AS-IS ########\n",
    "\n",
    "# get columns of interest\n",
    "numerical_columns = num_features.columns\n",
    "ordinal_columns = cat_features.filter(regex='Qual').columns\n",
    "categorical_columns = cat_features.drop(columns=ordinal_columns).columns\n",
    "\n",
    "# replace unique values in our ordinal columns (i.e. \"No_basement\", \"No_garage\") with 'NA'\n",
    "for col in ordinal_columns:\n",
    "    features[col] = features[col].replace(to_replace='No_.*', value='NA', regex=True)\n",
    "    \n",
    "# split full feature set (numeric, categorical, & ordinal features) into train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f1d19",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "######## RUN THIS CODE CELL AS-IS ########\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# create our numeric, categorical, and ordinal preprocessor encoders\n",
    "numerical_preprocessor = StandardScaler()\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "ordinal_categories = [\n",
    "    \"NA\", \"Very_Poor\", \"Poor\", \"Fair\", \"Below_Average\", \"Average\", \"Typical\",\n",
    "    \"Above_Average\", \"Good\", \"Very_Good\", \"Excellent\", \"Very_Excellent\"\n",
    "]\n",
    "list_of_ord_cats = [ordinal_categories for col in ordinal_columns]\n",
    "ordinal_preprocessor = OrdinalEncoder(categories=list_of_ord_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d4d19",
   "metadata": {},
   "source": [
    "2. Continued...\n",
    "\n",
    "Now fill in the blanks to create our `ColumnTransformer` that:\n",
    "\n",
    "- standardizes numerical columns (preprocessor: `numerical_preprocessor`; columns of interest: `numerical_columns`) \n",
    "- one-hot encodes categorical columns (preprocessor: `categorical_preprocessor`; columns of interest: `categorical_columns`) \n",
    "- ordinal encodes ordinal columns (preprocessor: `ordinal_preprocessor`; columns of interest: `ordinal_columns`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079666b",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('standard_scaler', __________, __________),\n",
    "    ('one_hot_encoder', __________, __________),\n",
    "    ('ordinal_encoder', __________, __________),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8487e",
   "metadata": {},
   "source": [
    "3. Now create a pipeline that includes the preprocessing step and applies a linear regression model. Does this improve the linear regression's $R^2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c4be5",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "lm_full = make_pipeline(___________, ___________)\n",
    "_ = lm_full.fit(X_train, y_train)\n",
    "lm_full.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174aabe",
   "metadata": {},
   "source": [
    "4. If time allows, create a pipeline that applies these preprocessing steps with a default random forest model and see if performance improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c4572-0ce6-4436-9621-835ee6b5c872",
   "metadata": {},
   "source": [
    "### GitHub Check-in\n",
    "\n",
    "Add, commit (with a good message!), and push your code to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f7037",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377cd13e",
   "metadata": {},
   "source": [
    "### Model evaluation & selection\n",
    "\n",
    "1. Using same preprocessing pipeline you created in Part 1, fit a default random forest model using a 5-fold cross validation procedure using the root mean squared error metric (`'neg_root_mean_squared_error'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5f15f",
   "metadata": {},
   "source": [
    "2. Run the following two code chunks as is without making any changes. This will create a random forest model pipeline and create specified hyperparameter distributions to draw from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294d7b1",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "######## RUN THIS CODE CELL AS-IS ########\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "class loguniform_int:\n",
    "    \"\"\"Integer valued version of the log-uniform distribution\"\"\"\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = loguniform(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"Random variable sample\"\"\"\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2903d1",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "######## RUN THIS CODE CELL AS-IS ########\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create preprocessor & modeling pipeline\n",
    "rf = RandomForestRegressor(random_state=123)\n",
    "pipeline = Pipeline([('prep', preprocessor), ('rf', rf)])\n",
    "\n",
    "# specify hyperparameter distributions to randomly sample from\n",
    "param_distributions = {\n",
    "    'rf__n_estimators': loguniform_int(50, 1000),\n",
    "    'rf__max_features': loguniform(.1, .8),\n",
    "    'rf__max_depth': loguniform_int(2, 30),\n",
    "    'rf__min_samples_leaf': loguniform_int(1, 100),\n",
    "    'rf__max_samples': loguniform(.5, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6771c59",
   "metadata": {},
   "source": [
    "2. Continued...\n",
    "\n",
    "Fill in the blanks to perform a random hyperparameter search based on the following:\n",
    "\n",
    "- use the parameter distributions specified above,\n",
    "- perform 25 random searches,\n",
    "- use a 5-fold cross-validation procedure, and\n",
    "- use root mean squared error (RMSE) as our scoring metric.\n",
    "\n",
    "What are the hyperparameters that provide the lowest RMSE? What is the lowest cross validated RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40410327",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import ___________\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline, \n",
    "    param_distributions=___________, \n",
    "    n_iter=__,\n",
    "    cv=__, \n",
    "    scoring='___________',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "results = random_search.___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed954c73-b660-4edc-93f4-b6869c0dc9d3",
   "metadata": {},
   "source": [
    "### Modular code & unit tests\n",
    "\n",
    "1. Move the `loguniform_int` class we defined above into a new module, `loguniform_int.py`. We haven't put classes into modules before, but it's no different than a function; just paste it along with any imports it needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd495e6-eb4b-4ddb-83b5-dde8e586ef36",
   "metadata": {},
   "source": [
    "Your new module should contain something like:\n",
    "\n",
    "```python\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "class loguniform_int:\n",
    "    \"\"\"Integer valued version of the log-uniform distribution\"\"\"\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = loguniform(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"Random variable sample\"\"\"\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375923a-f673-4c83-abd3-cf4f099a5b9c",
   "metadata": {},
   "source": [
    "2. Import your module and make sure you can use it in code by (re)running the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31f07d64-f468-4b4a-a60e-e338f2f00cb2",
   "metadata": {
    "tags": [
     "ci-skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "from loguniform_int import loguniform_int\n",
    "\n",
    "param_distributions = {\n",
    "    'rf__n_estimators': loguniform_int(50, 1000),\n",
    "    'rf__max_features': loguniform(.1, .8),\n",
    "    'rf__max_depth': loguniform_int(2, 30),\n",
    "    'rf__min_samples_leaf': loguniform_int(1, 100),\n",
    "    'rf__max_samples': loguniform(.5, 1),\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline, \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=10, # lower this to 10 so it's faster\n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "results2 = random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9dc10a-42dd-4cc4-b957-0451046cc5f9",
   "metadata": {},
   "source": [
    "3. Create a `tests.py` file in which you add the tests we already create for `get_features_and_target` (you can just copy them), along with a new test that asserts that `loguniform` objects have a `._distribution.args` attribute that holds the original numbers passed into them -- confirming that we did indeed create the kind of distribution we expected. Run the tests when finished.\n",
    "\n",
    "```python\n",
    ">>> lu = loguniform_int(2, 30)\n",
    ">>> lu._distribution.args\n",
    "(2, 30)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3dd7d-11c9-471f-8391-c5a23219acd6",
   "metadata": {},
   "source": [
    "4. Parametrize this test. Create one `loguniform_int` with `(2, 30)` as the arguments and another with `(1, 100)` as the arguments. Confirm that in both cases, the resulting `._distribution.args` attribute holds a tuple with the same numbers that were supplied initially. Rerun your tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98334504",
   "metadata": {},
   "source": [
    "### ML lifecycle management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8a06f",
   "metadata": {},
   "source": [
    "1. Create and set an MLflow experiment titled \"UC Advanced Python Case Study\"\n",
    "2. Re-perform the random hyperparameter search executed above while logging the hyperparameter search experiment with MLflow's autologging. Title this run \"rf_hyperparameter_tuning\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
